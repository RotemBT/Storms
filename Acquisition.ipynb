{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56U3bxpOZEV6"
   },
   "source": [
    "# **Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjeLISL_6ZM1"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import bs4\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFyhrgaYZKJ2"
   },
   "source": [
    "## **Oceans, Urls from wundergrund website and range of years for crawling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-J4fo0h6fJp"
   },
   "outputs": [],
   "source": [
    "# required years\n",
    "LAST_YEAR = 2021\n",
    "FIRST_YEAR = 1950\n",
    "# required oceans\n",
    "oceansURL = {\n",
    "    'Atlantic Ocean': \"https://www.wunderground.com/hurricane/archive/AL\",\n",
    "    'East Pacific': \"https://www.wunderground.com/hurricane/archive/EP\",\n",
    "    'Western Pacific': \"https://www.wunderground.com/hurricane/archive/WP\",\n",
    "    'Indian Ocean': \"https://www.wunderground.com/hurricane/archive/IO\",\n",
    "    'Central Pacific': \"https://www.wunderground.com/hurricane/archive/CP\",\n",
    "    'Southern Hemisphere': \"https://www.wunderground.com/hurricane/archive/SH\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_fohxh0ZjO5"
   },
   "source": [
    "# **Lists of records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWXM4E956kyr"
   },
   "outputs": [],
   "source": [
    "# required fields\n",
    "yearOfStorm = []\n",
    "oceans = []\n",
    "dates = []\n",
    "hours = []\n",
    "stormsName = []\n",
    "stormType = []\n",
    "latCorr = []\n",
    "longCorr = []\n",
    "windPower = []\n",
    "airPressure = []\n",
    "deaths = []\n",
    "damagedUsd = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wma44wp6Zuyu"
   },
   "source": [
    "**Chrome Driver Path** \n",
    "- Enter your path of your Chrome Driver. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAVDKMM26m04"
   },
   "outputs": [],
   "source": [
    "# Enter in chromeDriverPath your chrome driver path\n",
    "chromeDriverPath = \"\"\n",
    "s = Service(chromeDriverPath)\n",
    "driver = webdriver.Chrome(service=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aA5TE3K6fST2"
   },
   "source": [
    "# **Methods for crawling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umed0g2yaSAs"
   },
   "source": [
    "- sendToCSV - save the DataFrame to CSV file.\n",
    "- getDataFrame - return a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoNT7GXO6qLH"
   },
   "outputs": [],
   "source": [
    "def sendToCSV(dataFrame, fileName):\n",
    "    dataFrame.to_csv(fileName, index=False)\n",
    "    \n",
    "def getDataFrame(stormsName, yearOfStorm, oceans, dates, hours, windPower, airPressure, stormType, latCorr, longCorr):\n",
    "    return pd.DataFrame(\n",
    "        pd.DataFrame({'storm_name': stormsName, 'year': yearOfStorm, 'Ocean': oceans, 'date': dates, 'time': hours,\n",
    "                      'wind_power': windPower,\n",
    "                      'air_pressure': airPressure, 'storm_type': stormType, 'lat': latCorr, 'long': longCorr}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OErhYCw7apRU"
   },
   "source": [
    "- Get bs4 object of current URL using driver and bs4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9Y3h4jT6qGA"
   },
   "outputs": [],
   "source": [
    "def getSoupObj(url):\n",
    "    driver.get(url)\n",
    "    c = driver.page_source\n",
    "    return bs4.BeautifulSoup(c, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_T-VxBva08A"
   },
   "source": [
    "- xpath_soup - return a xpath of requested element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykGrBiWI6u9N"
   },
   "outputs": [],
   "source": [
    "def xpath_soup(element):\n",
    "    \"\"\"\n",
    "    Generate xpath of soup element\n",
    "    :param element: bs4 text or node\n",
    "    :return: xpath as string\n",
    "    \"\"\"\n",
    "    components = []\n",
    "    child = element if element.name else element.parent\n",
    "    for parent in child.parents:\n",
    "        \"\"\"\n",
    "        @type parent: bs4.element.Tag\n",
    "        \"\"\"\n",
    "        previous = itertools.islice(parent.children, 0, parent.contents.index(child))\n",
    "        xpath_tag = child.name\n",
    "        xpath_index = sum(1 for i in previous if i.name == xpath_tag) + 1\n",
    "        components.append(xpath_tag if xpath_index == 1 else '%s[%d]' % (xpath_tag, xpath_index))\n",
    "        child = parent\n",
    "    components.reverse()\n",
    "    return '/%s' % '/'.join(components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKbURx20bF-w"
   },
   "source": [
    "- getStormRecords - Getting records from current storm page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvW2zfFg6u6L"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def getStormRecords(soup, year, ocean, years, oceans, dates, hours, windPower, airPressure,\n",
    "                    stormType, stormNames, latCorr, longCorr):\n",
    "    # Sleep for wait for the dynamic page will be loaded.\n",
    "    time.sleep(2)\n",
    "    stormName = driver.find_element(By.CLASS_NAME, 'sub-header').text\n",
    "    stormName = stormName.split(' ')[1]\n",
    "    stormName = 'NOT_NAMED' if stormName == 'NOT' else stormName\n",
    "    try:\n",
    "        rows = soup.find('tbody').find_all('tr')\n",
    "    except:\n",
    "        rows = []\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        years.append(year)\n",
    "        oceans.append(ocean)\n",
    "        stormNames.append(stormName)\n",
    "        dates.append(columns[0].text)\n",
    "        hours.append(columns[1].text)\n",
    "        latCorr.append(columns[2].text)\n",
    "        longCorr.append(columns[3].text)\n",
    "        windPower.append(columns[4].text)\n",
    "        airPressure.append(columns[5].text)\n",
    "        stormType.append(columns[6].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6foNC11Jba46"
   },
   "source": [
    "- getInfoOfRow - getting information of specific row in specific year and ocean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KdajVLE65K7"
   },
   "outputs": [],
   "source": [
    "\n",
    "def getInfoOfRow(row, year, ocean, years, oceans, dates, hours, windPower, airPressure,\n",
    "                 stormType, stormNames, latCorr, longCorr):\n",
    "    columns = row.find_all('td')\n",
    "    startDate = str(columns[1].text).split(' - ')[1] + '/' + str(year)\n",
    "    years.append(year)\n",
    "    oceans.append(ocean)\n",
    "    stormNames.append(columns[0].text)\n",
    "    dates.append(startDate)\n",
    "    hours.append('12:00:00 PM')\n",
    "    latCorr.append(np.nan)\n",
    "    longCorr.append(np.nan)\n",
    "    windPower.append(columns[2].text)\n",
    "    airPressure.append(columns[3].text)\n",
    "    stormType.append(columns[4].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzNCL634blfY"
   },
   "source": [
    "- getGeneralRecord - getting records of storms in current year that we couldn't take the records from each storm, like years with 'Not_Named' storms in the beggining of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZQjMlBV65Ny"
   },
   "outputs": [],
   "source": [
    "def getGeneralRecord(year, ocean, years, oceans, dates, hours, windPower, airPressure,\n",
    "                     stormType, stormNames, latCorr, longCorr):\n",
    "    soup = getSoupObj(driver.current_url)\n",
    "    try:\n",
    "        rows = soup.find('tbody').find_all('tr')\n",
    "    except:\n",
    "        rows = []\n",
    "    for row in rows:\n",
    "        getInfoOfRow(row, year, ocean, years, oceans, dates, hours, windPower, airPressure,\n",
    "                     stormType, stormNames, latCorr, longCorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyuCVAY6cdZH"
   },
   "source": [
    "- scrapDataFromCurrYear - getting records of current year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rm0HI7T65Q5"
   },
   "outputs": [],
   "source": [
    "def scrapDataFromCurrYear(year, ocean, years, oceans, dates, hours, windPower, airPressure,\n",
    "                          stormType, stormNames, latCorr, longCorr, url):\n",
    "    try:\n",
    "        url = url + '/' + str(year)\n",
    "        driver.get(url)\n",
    "        # wait for the webpage will be load and to avoid huge overhead\n",
    "        time.sleep(4)\n",
    "        storm = driver.find_element(By.XPATH, '//*[@id=\"inner-content\"]/div[2]/div/div/div[2]/div/div[3]'\n",
    "                                              '/lib-storms-list/div/div/div[2]/div/div/table/tbody/tr[1]/td[1]/a')\n",
    "        \n",
    "        # if the page doesnt start with 'NOT_NAMED' storm_name then click on the first storm and continue until there is no storms left.\n",
    "        if storm.text != 'NOT_NAMED' and storm.text != ' NOT_NAMED ':\n",
    "            storm.click()\n",
    "            soup = getSoupObj(driver.current_url)\n",
    "            # if there is a table in the records then continue to the loop.\n",
    "            if soup.find('table') is not None:\n",
    "                while True:\n",
    "                    soup = getSoupObj(driver.current_url)\n",
    "                    getStormRecords(soup, year, ocean, years, oceans, dates, hours, windPower, airPressure,\n",
    "                                    stormType, stormNames, latCorr, longCorr)\n",
    "                    try:\n",
    "                        nextStorm = driver.find_element(By.XPATH,\n",
    "                                                        '//*[@id=\"inner-content\"]/div[2]/div/div/div[2]/div/div['\n",
    "                                                        '2]/lib-storm/div/div/div[1]/ul/li[3]/a')\n",
    "                        if nextStorm.text != 'All Storms Â»':\n",
    "                            nextStorm.click()\n",
    "                        else:\n",
    "                            break\n",
    "                    except:\n",
    "                        break\n",
    "            #in case there is no table back to previous url and take the general records and continue to the next storm.\n",
    "            else:\n",
    "                soup = getSoupObj(url)\n",
    "                rows = soup.find('tbody').find_all('tr')\n",
    "                getInfoOfRow(rows[0], year, ocean, years, oceans, dates, hours, windPower, airPressure,\n",
    "                             stormType, stormNames, latCorr, longCorr)\n",
    "                tdOfStorm = soup.find_all('td', {\n",
    "                    'class': 'mat-cell cdk-cell cdk-column-summaryStormName mat-column-summaryStormName ng-star-inserted'})\n",
    "                stormsLink = [s.find('a') for s in tdOfStorm]\n",
    "                for i in range(1, len(stormsLink)):\n",
    "                    driver.get(url)\n",
    "                    time.sleep(4)\n",
    "                    xpath = xpath_soup(stormsLink[i])\n",
    "                    nextStorm = stormsLink[i]\n",
    "                    #if the name of storm is NOT_NAMED take the general record.\n",
    "                    if nextStorm.text == ' NOT_NAMED ' or nextStorm.text == 'NOT_NAMED':\n",
    "                        getInfoOfRow(rows[i], year, ocean, years, oceans, dates, hours, windPower, airPressure,\n",
    "                                     stormType, stormNames, latCorr, longCorr)\n",
    "                        continue\n",
    "\n",
    "                    # moving to next page by clicking on link text (with selenium)\n",
    "                    element = driver.find_element(By.XPATH, xpath)\n",
    "                    element.click()\n",
    "                    soup = getSoupObj(driver.current_url)\n",
    "                    haveTable = soup.find('table')\n",
    "                    if haveTable is not None:\n",
    "                        getStormRecords(soup, year, ocean, years, oceans, dates, hours, windPower, airPressure,\n",
    "                                        stormType, stormNames, latCorr, longCorr)\n",
    "                    else:\n",
    "                        soup = getSoupObj(url)\n",
    "                        time.sleep(5)\n",
    "                        rows = soup.find('tbody').find_all('tr')\n",
    "                        getInfoOfRow(rows[i], year, ocean, years, oceans, dates, hours, windPower, airPressure,\n",
    "                                     stormType, stormNames, latCorr, longCorr)\n",
    "        #in case the first storm is NOT_NAMED take the general records of this year.\n",
    "        else:\n",
    "            soup = getSoupObj(driver.current_url)\n",
    "            try:\n",
    "                rows = soup.find('tbody').find_all('tr')\n",
    "            except:\n",
    "                rows = []\n",
    "            for row in rows:\n",
    "                getInfoOfRow(row, year, ocean, years, oceans, dates, hours, windPower, airPressure,\n",
    "                             stormType, stormNames, latCorr, longCorr)\n",
    "    except:\n",
    "        print(driver.current_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOIX8zSycxhX"
   },
   "source": [
    "- scrapData - crawling each Ocean and year in range and export to CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idxgHYr26vAm"
   },
   "outputs": [],
   "source": [
    "def scrapData():\n",
    "    for ocean, url in oceansURL.items():\n",
    "        for i in range(LAST_YEAR, FIRST_YEAR, -1):\n",
    "            scrapDataFromCurrYear(i, ocean, yearOfStorm, oceans, dates, hours, windPower, airPressure,\n",
    "                                  stormType, stormsName, latCorr, longCorr, url)\n",
    "\n",
    "    driver.quit()\n",
    "    df = getDataFrame(stormsName, yearOfStorm, oceans, dates, hours, windPower, airPressure, stormType, latCorr,\n",
    "                      longCorr)\n",
    "    sendToCSV(df, 'storms.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4QsVoqrdOR0"
   },
   "source": [
    "# **Run the main function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPj-Bdeh8Wl3"
   },
   "outputs": [],
   "source": [
    "scrapData()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "56U3bxpOZEV6",
    "e4QsVoqrdOR0"
   ],
   "name": "Acquisition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
